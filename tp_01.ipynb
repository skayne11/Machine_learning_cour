{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me_data = pd.read_csv('./dataset/melb_data.csv')\n",
    "me_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10864, 13580]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 65\u001b[0m\n\u001b[0;32m     55\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     56\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     57\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Entraîner le modèle avec GridSearchCV\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Meilleurs paramètres\u001b[39;00m\n\u001b[0;32m     68\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\Skayne\\Desktop\\cours_tech\\Machine learning\\env\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Skayne\\Desktop\\cours_tech\\Machine learning\\env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:922\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    919\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator\n\u001b[0;32m    920\u001b[0m scorers, refit_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_scorers()\n\u001b[1;32m--> 922\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    923\u001b[0m params \u001b[38;5;241m=\u001b[39m _check_method_params(X, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m    925\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_routed_params_for_fit(params)\n",
      "File \u001b[1;32mc:\\Users\\Skayne\\Desktop\\cours_tech\\Machine learning\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <3x1 sparse matrix ...>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Skayne\\Desktop\\cours_tech\\Machine learning\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10864, 13580]"
     ]
    }
   ],
   "source": [
    "# Imputation des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "me_data[me_data.select_dtypes(include=[np.number]).columns] = imputer.fit_transform(me_data.select_dtypes(include=[np.number]))\n",
    "\n",
    "# Encodage par Target Encoding\n",
    "if 'Suburb' in me_data.columns:\n",
    "    suburb_mean_price = me_data.groupby('Suburb')['Price'].mean()\n",
    "    me_data['Suburb_encoded'] = me_data['Suburb'].map(suburb_mean_price)\n",
    "    me_data.drop(columns=['Suburb'], inplace=True)\n",
    "\n",
    "# Création de caractéristiques supplémentaires\n",
    "me_data['Landsize_squared'] = me_data['Landsize'] ** 2\n",
    "me_data['BuildingArea_log'] = np.log1p(me_data['BuildingArea'])\n",
    "me_data['Rooms_Bathroom'] = me_data['Rooms'] * me_data['Bathroom']\n",
    "\n",
    "# Préparation des données\n",
    "y = me_data['Price']\n",
    "fme_features = [\n",
    "    'Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt',\n",
    "    'Lattitude', 'Longtitude', 'Suburb_encoded',\n",
    "    'Landsize_squared', 'BuildingArea_log', 'Rooms_Bathroom'\n",
    "]\n",
    "X = me_data[fme_features]\n",
    "\n",
    "# Discrétisation de la cible\n",
    "y_binned = pd.qcut(y, q=5, labels=False)\n",
    "\n",
    "# Normalisation\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Nettoyage des indices invalides\n",
    "valid_indices = ~pd.isna(y_binned)\n",
    "X_scaled = X_scaled[valid_indices]\n",
    "y = y.reset_index(drop=True)[valid_indices]\n",
    "y_binned = y_binned.reset_index(drop=True)[valid_indices].astype(int)\n",
    "\n",
    "# Validation des tailles\n",
    "print(f\"X_scaled: {X_scaled.shape}, y: {y.shape}, y_binned: {y_binned.shape}\")\n",
    "\n",
    "# Validation des catégories\n",
    "print(f\"Distribution dans y_binned: \\n{y_binned.value_counts()}\")\n",
    "\n",
    "# Paramètres pour GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Modèle Random Forest\n",
    "model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# GridSearchCV avec 5-Fold CV (non stratifié)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=5,  # 5-fold CV standard\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Séparation des données\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraînement du modèle\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Résultats\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Performances\n",
    "print(f\"Best MAE (Positive): {-grid_search.best_score_:.2f}\")\n",
    "preds = best_model.predict(X_test)\n",
    "mae_final = mean_absolute_error(y_test, preds)\n",
    "print(f\"Final Mean Absolute Error on test data: {mae_final:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Mean Absolute Error: 226925.9660191458\n"
     ]
    }
   ],
   "source": [
    "# Imputation des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "me_data[me_data.select_dtypes(include=[np.number]).columns] = imputer.fit_transform(me_data.select_dtypes(include=[np.number]))\n",
    "\n",
    "# Encoder les colonnes catégoriques via Target Encoding\n",
    "if 'Suburb' in me_data.columns:\n",
    "    suburb_mean_price = me_data.groupby('Suburb')['Price'].mean()\n",
    "    me_data['Suburb_encoded'] = me_data['Suburb'].map(suburb_mean_price)\n",
    "    me_data.drop(columns=['Suburb'], inplace=True)\n",
    "\n",
    "# Créer des caractéristiques supplémentaires\n",
    "me_data['Landsize_squared'] = me_data['Landsize'] ** 2\n",
    "me_data['BuildingArea_log'] = np.log1p(me_data['BuildingArea'])\n",
    "me_data['Rooms_Bathroom'] = me_data['Rooms'] * me_data['Bathroom']\n",
    "\n",
    "# Sélectionner les caractéristiques importantes et la cible\n",
    "y = me_data['Price']\n",
    "fme_features = [\n",
    "    'Suburb_encoded', 'Rooms', 'Longtitude', 'Lattitude', \n",
    "    'Landsize_squared', 'Landsize', 'Rooms_Bathroom',\n",
    "    'YearBuilt', 'BuildingArea_log', 'BuildingArea', 'Bathroom', 'Postcode', 'Bedroom2', 'Distance'\n",
    "]\n",
    "\n",
    "X = me_data[fme_features]\n",
    "\n",
    "# Normalisation des données\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs paramètres\n",
    "best_params = {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 1000}\n",
    "model = RandomForestRegressor(\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    bootstrap= False,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "preds = model.predict(X_test)\n",
    "mae_final = mean_absolute_error(y_test, preds)\n",
    "print(\"Final Mean Absolute Error:\", mae_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
